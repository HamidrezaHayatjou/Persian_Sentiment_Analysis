{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xs_WkCUTydcF",
        "fB8YM0Z6bjZn",
        "I8H-D0fFcBWv",
        "DjgU_xHx1IGW",
        "tzRR3wm80mnF",
        "bq53SLPG0uTe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##clone repo"
      ],
      "metadata": {
        "id": "-lFZbA5kyiks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKRBNG9TTDiE"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/HamidrezaHayatjou/Persian_Sentiment_Analysis.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install and import libs"
      ],
      "metadata": {
        "id": "Xs_WkCUTydcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install clean-text\n",
        "# after installing please Restart Runtime"
      ],
      "metadata": {
        "id": "A-KmBKWSYrXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import stats as s\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,f1_score)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from statistics import mode\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense, GRU, Embedding, LSTM, Dropout,Conv1D\n",
        "from tensorflow.keras.layers import BatchNormalization, GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "\n",
        "from cleantext.clean import clean\n",
        "from hazm import *\n",
        "import hazm\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o3iG7jOZMGb",
        "outputId": "fa84a5a4-fd2b-41ce-e335-86e85cf9fddc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading and exploring digikala comment dataset"
      ],
      "metadata": {
        "id": "fB8YM0Z6bjZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd Persian_Sentiment_Analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlcwqjTRZJZv",
        "outputId": "4aa02f36-c8c9-476e-f7da-f3999c2b4e33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Persian_Sentiment_Analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('digikala_comment_dataset.xlsx')\n",
        "df = df[[\"recommend\", \"comment\"]]\n",
        "df.rename(columns = {'recommend':'LABEL'}, inplace = True)\n",
        "df.rename(columns = {'comment':'COMMENT'}, inplace = True)\n",
        "print(df.shape) #(100000, 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BGy1eqrZZFn",
        "outputId": "b901ed1f-1a71-4bc7-c4db-6446b8a91bf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['LABEL'].value_counts())\n",
        "print(\"-------------------------\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "9y2gd9uxZ1pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['LABEL', 'COMMENT'], inplace=True)\n",
        "print(df.isnull().sum())\n",
        "print(\"-------------------------\")\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ulVYy3waAkJ",
        "outputId": "31507198-31d0-4431-c1b5-78bb8f79fac4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL      0\n",
            "COMMENT    0\n",
            "dtype: int64\n",
            "-------------------------\n",
            "(99883, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df.LABEL.isin([\"\\\\N\"])]\n",
        "print(df.shape)\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "QZZOM6KKaGw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_names = df.LABEL.unique()\n",
        "LABEL_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF1_KlXbaTHh",
        "outputId": "b02885b0-b056-4e23-f545-8ca1180094ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['recommended', 'not_recommended', 'no_idea'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id= {'recommended': 0, 'not_recommended': 1,'no_idea': 2}\n",
        "id2label= {0: 'recommended', 1: 'not_recommended', 2:'no_idea'}\n",
        "\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FxRd4uKaavN",
        "outputId": "afda6356-8e04-4d42-8928-31e050c58872"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label2id: {'recommended': 0, 'not_recommended': 1, 'no_idea': 2}\n",
            "id2label: {0: 'recommended', 1: 'not_recommended', 2: 'no_idea'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['LABEL'] = df.LABEL.replace(label2id)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l9_va0RgamaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentiment_labels = ['recommended', 'not_recommended','no_idea']\n",
        "label_counts = [0 for e in Sentiment_labels]\n",
        "labelcount = df['LABEL'].value_counts()\n",
        "label_counts = label_counts\n",
        "\n",
        "plt.bar(Sentiment_labels, labelcount)\n",
        "plt.xlabel('Sentiments')\n",
        "plt.ylabel('Number of comments for each label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VavkJ_8jaz3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preproces"
      ],
      "metadata": {
        "id": "I8H-D0fFcBWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"comment_len_by_words\"] = np.nan\n",
        "for i in tqdm(range (0, len(df['COMMENT']))):\n",
        "  df['comment_len_by_words'][i] = len(hazm.word_tokenize(str(df['COMMENT'][i])))\n",
        "\n",
        "#df['comment_len_by_words'] = df['COMMENT'].apply(lambda t: len(hazm.word_tokenize(str(t))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvA9UwU5cCEb",
        "outputId": "18f462fb-4872-4356-fc4f-bdd39d51e1ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/63586 [00:00<?, ?it/s]<ipython-input-13-fb36d9cc89dd>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['comment_len_by_words'][i] = len(hazm.word_tokenize(str(df['COMMENT'][i])))\n",
            "100%|██████████| 63586/63586 [00:27<00:00, 2306.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = df[\"comment_len_by_words\"].min()\n",
        "max_len = df[\"comment_len_by_words\"].max()\n",
        "print(f'Minimum: {min_len} \\tMaximum: {max_len}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R38OmQVgc0mE",
        "outputId": "54f9e15b-21d4-49c7-e58a-f873591f32c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum: 1.0 \tMaximum: 985.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):\n",
        "\n",
        "    data_length = data[col].values\n",
        "    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n",
        "    data_glt_rate = (data_glt / len(data_length)) * 100\n",
        "    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')\n",
        "\n",
        "data_gl_than(df, 256, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4hMDt9OdQoJ",
        "outputId": "28f5038f-6a70-4733-b114-1571efa7ea53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts with word length of greater than 3 and less than 256 includes 94.49% of the whole!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove comments with the length of fewer than three words\n",
        "minlim, maxlim = 3, 256\n",
        "df['comment_len_by_words'] = df['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
        "df = df.dropna(subset=['comment_len_by_words'])\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NLpshKPTd5kU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Histogram(x=df['comment_len_by_words']))\n",
        "fig1.update_layout(title_text='Distribution of word counts within comments', xaxis_title_text='Word Count', yaxis_title_text='Frequency', bargap=0.2, bargroupgap=0.2)\n",
        "\n",
        "fig2 = go.Figure()\n",
        "groupby_rate = df.groupby('LABEL')['LABEL'].count()\n",
        "fig2.add_trace(go.Bar(x=list(sorted(groupby_rate.index)), y=groupby_rate.tolist(), text=groupby_rate.tolist(), textposition='auto'))\n",
        "fig2.update_layout(title_text='Distribution of rate within comments', xaxis_title_text='Rate', yaxis_title_text='Frequency',bargap=0.2, bargroupgap=0.2)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ],
      "metadata": {
        "id": "Cv7wilReeGF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to data imbalance, we throw away part of the data.**"
      ],
      "metadata": {
        "id": "3kdrCVypzoXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df=df.sort_values(by=['LABEL'])\n",
        "sorted_df.reset_index(drop=True, inplace=True)\n",
        "# print(sorted_df.shape)\n",
        "# sorted_df\n",
        "sorted_df['LABEL'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clsnKsjCiTYv",
        "outputId": "3cbd1626-7990-41d5-c52a-39120d12cbed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    34708\n",
              "1    15306\n",
              "2    10069\n",
              "Name: LABEL, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sorted_df['LABEL'][0:10069]))\n",
        "print(len(sorted_df['LABEL'][34708:44777]))\n",
        "print(len(sorted_df['LABEL'][50014:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj8c-dqZiWk4",
        "outputId": "9925a025-0e01-4483-89ec-7cab1e073168"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10069\n",
            "10069\n",
            "10069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([sorted_df[0:10069], sorted_df[34708:44777], sorted_df[50014:]])\n",
        "data = shuffle(data)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "print(len(data))\n",
        "data"
      ],
      "metadata": {
        "id": "OHWCrsM3iqgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning data**"
      ],
      "metadata": {
        "id": "f2QA9xMiz5oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function borrowed from https://colab.research.google.com/github/hooshvare/parsbert/blob/master/notebooks/Taaghche_Sentiment_Analysis.ipynb\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # regular cleaning\n",
        "    text = clean(text, fix_unicode=True, to_ascii=False, lower=True, no_line_breaks=True, no_urls=True, no_emails=True,\n",
        "                 no_phone_numbers=True, no_numbers=False, no_digits=False, no_currency_symbols=True, no_punct=False,\n",
        "                 replace_with_url=\"\", replace_with_email=\"\", replace_with_phone_number=\"\", replace_with_number=\"\", replace_with_digit=\"0\",\n",
        "                 replace_with_currency_symbol=\"\",)\n",
        "\n",
        "    # cleaning htmls\n",
        "    text = cleanhtml(text)\n",
        "\n",
        "    # normalizing\n",
        "    normalizer = hazm.Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "\n",
        "    # removing wierd patterns\n",
        "    wierd_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2069\"\n",
        "        u\"\\u2066\"\n",
        "        # u\"\\u200c\"\n",
        "        u\"\\u2068\"\n",
        "        u\"\\u2067\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = wierd_pattern.sub(r'', text)\n",
        "\n",
        "    # removing extra spaces, hashtags\n",
        "    text = re.sub(\"#\", \"\", text)\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "N99rj6nSiO03"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes about 7 hours to run this cell. Therefore, you can import the cleared data by running the next cell."
      ],
      "metadata": {
        "id": "tYP3H_3G0CKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It takes about 7 hours to run this cell. Therefore, you can import the cleared data by running the next cell.\n",
        "data[\"cleaned_comment\"] = np.nan\n",
        "for i in tqdm(range (0, len(data['COMMENT']))):\n",
        "  data['cleaned_comment'][i] = cleaning(str(data['COMMENT'][i]))\n",
        "\n",
        "data = data[['cleaned_comment', 'LABEL']]\n",
        "data.columns = ['COMMENT', 'LABEL']\n",
        "data.to_excel(\"cleaned_digikala_dataset.xlsx\", index=False)\n",
        "# data.head()"
      ],
      "metadata": {
        "id": "b4YzB8PHk893"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/Persian_Sentiment_Analysis/cleaned_digikala_dataset.xlsx')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DUlTTWkRnhDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "groupby_label = data.groupby('LABEL')['LABEL'].count()\n",
        "fig.add_trace(go.Bar(x=list(sorted(groupby_label.index)), y=groupby_label.tolist(), text=groupby_label.tolist(), textposition='auto'))\n",
        "fig.update_layout(title_text='Distribution of label within comments', xaxis_title_text='Label', yaxis_title_text='Frequency', bargap=0.2, bargroupgap=0.2)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QIY_GLcLtl0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenizing, vectorizing, splitting data"
      ],
      "metadata": {
        "id": "DjgU_xHx1IGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(0, len(data))\n",
        "sample_comment = data.iloc[idx]['COMMENT']\n",
        "sample_label = data.iloc[idx]['LABEL']\n",
        "\n",
        "print(f'Sample: \\n{sample_comment}\\n{sample_label}')"
      ],
      "metadata": {
        "id": "6OhlzWhRt6f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = data['LABEL']\n",
        "text = data['COMMENT']\n",
        "Xtrain = text\n",
        "Ytrain= target\n",
        "print(Xtrain.shape)"
      ],
      "metadata": {
        "id": "O6VW8pDQt8Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Ytrain = data['COMMENT'], data['LABEL']\n",
        "print(Xtrain.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB92We4kmLWv",
        "outputId": "40a7a119-8b1a-4ed4-e313-9607278014be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30207,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=' '.join(Xtrain)\n",
        "all_words=word_tokenize(all_words)\n",
        "dist=FreqDist(all_words)\n",
        "num_unique_word=len(dist)\n",
        "print ('number unique word:',num_unique_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APLPbTZMmsQJ",
        "outputId": "e10de2e5-ea26-4f92-e4cf-9820e0b11bf9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number unique word: 37005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 100\n",
        "num_words = num_unique_word\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(Xtrain)\n",
        "\n",
        "x_train_tokens = tokenizer.texts_to_sequences(Xtrain)\n",
        "\n",
        "#print(Xtrain[100])\n",
        "print(x_train_tokens[100])\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
        "\n",
        "\n",
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "def tokens_to_string(tokens):\n",
        "\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "print('train shape',x_train_pad.shape)"
      ],
      "metadata": {
        "id": "MIH0d8dDm3KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_train, txt_test, lbl_train, lbl_test = train_test_split(x_train_pad, Ytrain,test_size=0.1)\n",
        "txt_train, txt_valid, lbl_train, lbl_valid = train_test_split(txt_train, lbl_train, test_size=0.2)\n",
        "\n",
        "x_train, y_train = txt_train.tolist(), lbl_train.tolist()\n",
        "x_valid, y_valid = txt_valid.tolist(), lbl_valid.tolist()\n",
        "x_test, y_test = txt_test.tolist(), lbl_test.tolist()"
      ],
      "metadata": {
        "id": "Q-fGofJDn_ja"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model definition and training"
      ],
      "metadata": {
        "id": "tzRR3wm80mnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 250\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_size, input_length=max_tokens, name='embedding_layer'))\n",
        "# model.add(Dropout(0.25))\n",
        "model.add(Conv1D(256,kernel_size=3,padding='same',activation='elu',strides=1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(200, activation='elu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aTGx7i6OqzKg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_data=(x_valid, y_valid), verbose=1)"
      ],
      "metadata": {
        "id": "hc8BU2DJq2lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##predicting"
      ],
      "metadata": {
        "id": "bq53SLPG0uTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHcRx_lhq5QS",
        "outputId": "f9e10984-7247-439b-b1b6-da58efbe0231"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95/95 [==============================] - 3s 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred , average=\"micro\")\n",
        "precision = precision_score(y_test, y_pred , average=\"micro\")\n",
        "f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
        "\n",
        "print(\"accuracy:\",\"%.2f\" %(accuracy*100))\n",
        "print(\"racall:\",\"%.2f\" %(recall*100))\n",
        "print(\"precision:\",\"%.2f\" %(precision*100))\n",
        "print(\"f1score:\",\"%.2f\" %(f1*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZZDZK9dq7Mt",
        "outputId": "1f72175b-61e3-4ee8-aa0a-b9f126f54345"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 62.83\n",
            "racall: 62.83\n",
            "precision: 62.83\n",
            "f1score: 62.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl7GrNXEwU2D",
        "outputId": "164a389f-98bf-406b-aec5-256873814eff"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'recommended', 1: 'not_recommended', 2: 'no_idea'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0,5):\n",
        "  a = x_test[i]\n",
        "  print(a)\n",
        "  text = tokenizer.sequences_to_texts([a])\n",
        "  print(text)\n",
        "  print('true label is : ', y_test[i])\n",
        "  print('pred label is : ', y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_kj6gLpv9-U",
        "outputId": "a33b1c36-4b66-448f-e692-2810cf7ce263"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9, 289, 1190, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['خیلی بدرد نخور بود']\n",
            "true label is :  1\n",
            "pred label is :  1\n",
            "[381, 1, 1730, 417, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['قشنگه و مث عکسش بود']\n",
            "true label is :  2\n",
            "pred label is :  2\n",
            "[123, 33699, 4, 85, 80, 348, 31, 153, 2068, 3723, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['همین امرو به دستم رسید کاش فقط یکم جعبش قشنگ\\u200cتر بود']\n",
            "true label is :  0\n",
            "pred label is :  0\n",
            "[75, 1891, 8748, 6, 4439, 12, 11, 128, 320, 91, 50, 540, 585, 1066, 116, 1, 2646, 116, 14, 55, 4, 60, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['سلام تونستم ۱۲۰۰ d کنون رو با کمی فشار بیشتر روی پیچ تنظیم ثابت کنم و عکاسی کنم در کل به قیمتش عالیه']\n",
            "true label is :  0\n",
            "pred label is :  0\n",
            "[10, 8, 47, 12, 37, 488, 74, 1, 9, 82, 132, 38, 1011, 607, 215, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['من این محصول رو یک ماهه گرفتم و خیلی ازش راضیم واقعا پوستم تغییر کرده']\n",
            "true label is :  0\n",
            "pred label is :  0\n"
          ]
        }
      ]
    }
  ]
}